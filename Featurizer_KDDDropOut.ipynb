{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "###################################################################################\n",
    "# Script configuration section\n",
    "###################################################################################\n",
    "\n",
    "# If you have a timestamp column, you can enable time features expansion to \n",
    "# get extra time-related features like hour, day, hour_of_day, day_of_week, etc.\n",
    "enable_time_features_expansion = True\n",
    "\n",
    "# timestamp_column is the name of the column containing unix timestamp\n",
    "# This column is used to expand time-related features\n",
    "timestamp_column = 'time'\n",
    "\n",
    "# vector_keys is the list of keys (column names) with significance \n",
    "# in the problem domain (e.g. enrollment_id, user_id, course_id)\n",
    "# For each key, a feature vector is calculated. These vectors are all merged\n",
    "# to create the final feature vector output.\n",
    "vector_keys = ['eid', 'cid', 'uid']\n",
    "\n",
    "# groupby_keys is the list of keys (column names) used in groupby operation.\n",
    "# For each key, a group of new features is calculated, using that key for grouping.\n",
    "groupby_keys = ['src_evt', 'cat']\n",
    "\n",
    "# categorical_columns is the list of columns that are aggregated using categorical_aggregators.\n",
    "categorical_columns = vector_keys + groupby_keys + ['obj']\n",
    "\n",
    "# categorical_aggregators is the list of aggregator functions used on categorical_columns.\n",
    "# Each aggregator function yields a new feature.\n",
    "categorical_aggregators = ['unique']\n",
    "\n",
    "# numeric_columns is the list of columns that are aggregated using numeric_aggregators.\n",
    "numeric_columns = [timestamp_column]\n",
    "\n",
    "# numeric_aggregators is the list of aggregator functions used on numeric_columns.\n",
    "# Each aggregator function yields a new feature.\n",
    "numeric_aggregators = ['count', 'min', 'max', 'span']\n",
    "\n",
    "# enable_subtraction_features calculates features resulting from\n",
    "# subtraction of features vectors for a pair vector keys (e.g. eid-uid)\n",
    "# If such operation doesn't make sense for your problem, disable this option.\n",
    "enable_subtraction_features = True\n",
    "\n",
    "###################################################################################\n",
    "# Script main body\n",
    "###################################################################################\n",
    "\n",
    "output = {}\n",
    "\n",
    "\n",
    "# Add extra time-related features based on the timestamp column\n",
    "# New featuers are added in-place\n",
    "def expand_time_features(data):\n",
    "    # timestamp is in seconds (unix timestamp http://www.unixtimestamp.com/)\n",
    "    # Features below are monotonically increasing with time, i.e. they don't\n",
    "    # wrap around with respect to year, month, or week\n",
    "    data[timestamp_column] = data[timestamp_column].apply(lambda t: int(t))\n",
    "    data['hour'] = data[timestamp_column].apply(lambda x: int(x / 3600))\n",
    "    data['day']  = data['hour'].apply(lambda x: int(x / 24))\n",
    "    data['week'] = data['day'].apply(lambda x: int(x / 7))\n",
    "\n",
    "    # Features below wrap around with respect to year, week, or day. \n",
    "    # All the features are 1-based\n",
    "    data['hour_of_day'] = data['hour'].apply(lambda x: (x % 24) + 1)\n",
    "    data['day_of_week'] = data['day'].apply(lambda x: (x % 7) + 1)\n",
    "    data['week_of_year'] = data['week'].apply(lambda x: (x % 52) + 1)\n",
    "\n",
    "    # Since we have time as numeric feature, it doesn't add much information to make new features numeric.\n",
    "    # So we just add new features as categorical.\n",
    "    categorical_columns.extend(['hour', 'day', 'week', 'hour_of_day', 'day_of_week', 'week_of_year'])\n",
    "   \n",
    "    # Add day_of_week as a new groupby key\n",
    "    groupby_keys.append('day_of_week')\n",
    "\n",
    "\n",
    "def span(arr):\n",
    "    return arr.max() - arr.min()\n",
    "\n",
    "\n",
    "def unique(arr):\n",
    "    return pd.Series.nunique(arr)\n",
    "\n",
    "def replace_names_in_list(input_list, replace_dict):\n",
    "    for name, replacement in replace_dict.items():\n",
    "        try:\n",
    "            i = input_list.index(name)\n",
    "            input_list[i] = replacement\n",
    "        except:\n",
    "          pass          \n",
    "\n",
    "     \n",
    "def create_aggregation_groups():\n",
    "    # Replace custome aggregator names with actual functions.\n",
    "    # String names are used in the the config section, b/c I wanted all the declarations to be below config.\n",
    "    mapping = {'unique': unique, 'span': span}\n",
    "    replace_names_in_list(categorical_aggregators, mapping)\n",
    "    replace_names_in_list(numeric_aggregators, mapping)\n",
    "\n",
    "    # Single-key aggreagtion group is for aggregation with one vector key, without any groupby key, hence single-key.\n",
    "    # For single-key group, we perform both numeric and categorical aggregations.\n",
    "    single_key_aggregation = [(col,categorical_aggregators) for col in categorical_columns]\n",
    "    single_key_aggregation = single_key_aggregation + [(col,numeric_aggregators) for col in numeric_columns]\n",
    "    single_key_aggregation_dict = dict(single_key_aggregation)\n",
    "\n",
    "    # Double-key aggregation group is for aggregation using a vector key in combination with a groupby key, hence double-key.\n",
    "    # For these aggregations, we use only numeric aggregations, to keep the number of features from exploding.\n",
    "    double_key_aggregation_dict = dict((col,numeric_aggregators) for col in numeric_columns)\n",
    "\n",
    "    aggregation_groups = [([], single_key_aggregation_dict)]\n",
    "    for key in groupby_keys:\n",
    "        aggregation_groups.append(([key], double_key_aggregation_dict))\n",
    "\n",
    "    return aggregation_groups\n",
    "\n",
    "\n",
    "def compute_group_features(data, group_keys, aggregation_dict):\n",
    "    grouped_data = data.groupby(group_keys)\n",
    "    agg_data = grouped_data.agg(aggregation_dict)\n",
    "    \n",
    "    if (len(group_keys) == 1):\n",
    "        unstacked_data = agg_data\n",
    "        formatted_columns = ['_'.join(col).strip() for col in unstacked_data.columns.values]\n",
    "    else:\n",
    "        unstacked_data = agg_data.unstack().reorder_levels([2,0,1], axis=1)\n",
    "        column_prefix = '{0}='.format(group_keys[1])\n",
    "        formatted_columns = [column_prefix + str(col[0]) + '|' + '_'.join(col[1:]).strip() for col in unstacked_data.columns.values]\n",
    "\n",
    "    #flatten the hierarchical column index\n",
    "    unstacked_data.columns = formatted_columns\n",
    "    unstacked_data.sort_index(axis=1, inplace=True)\n",
    "    return unstacked_data\n",
    "\n",
    "\n",
    "def compute_vector_key_features(data, aggregation_groups):\n",
    "    for vector_key in vector_keys:\n",
    "        feature_groups = []\n",
    "        for item in aggregation_groups:\n",
    "            group = [vector_key] + item[0]\n",
    "            aggregation_dict = item[1]\n",
    "            features = compute_group_features(data, group, aggregation_dict)\n",
    "            feature_groups.append(features) \n",
    "\n",
    "        vector_key_features = pd.concat(feature_groups, axis=1)\n",
    "        output[vector_key] = vector_key_features\n",
    "\n",
    "\n",
    "def add_subtraction_features(vector_keys_mapping):\n",
    "    joined_output = {}\n",
    "    for key, vector in output.items():\n",
    "        joined_output[key] = output[key].reset_index().merge(vector_keys_mapping, how='inner').set_index(vector_keys)\n",
    "\n",
    "    # Get all the combinations of two vector_keys\n",
    "    for i, key_1 in enumerate(vector_keys[:-1]):\n",
    "        for key_2 in vector_keys[i+1:]:\n",
    "            id = '_{0}-{1}'.format(key_1, key_2)\n",
    "            output[id] = joined_output[key_1].sub(joined_output[key_2])\n",
    "            \n",
    "\n",
    "    \n",
    "def Featurize(data = None, ignore = None):\n",
    "    if enable_time_features_expansion:\n",
    "        expand_time_features(data)\n",
    "    CountWeeklyEvents(data)\n",
    "    aggregation_groups = create_aggregation_groups()\n",
    "\n",
    "    vector_keys_mapping = data[vector_keys]\n",
    "    vector_keys_mapping = vector_keys_mapping.drop_duplicates()\n",
    "\n",
    "    compute_vector_key_features(data, aggregation_groups)\n",
    "\n",
    "    if enable_subtraction_features:\n",
    "        add_subtraction_features(vector_keys_mapping)\n",
    "\n",
    "    output_vector = vector_keys_mapping\n",
    "    for key, vector in output.items():\n",
    "        # Fix column names before final merge\n",
    "        key_prefix = 'key={0}|'.format(key)\n",
    "        vector = vector.add_prefix(key_prefix)\n",
    "        vector.reset_index(inplace=True)\n",
    "        output[key] = vector\n",
    "\n",
    "        output_vector = output_vector.merge(vector, how='left')\n",
    "\n",
    "    return output_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# Section for local testing. Will be deleted in the final results.\n",
    "###################################################################################\n",
    "\n",
    "debug = False\n",
    "debug_rows = 1000\n",
    "add_smart_features = True\n",
    "add_rank_features = False\n",
    "add_norm_features = False\n",
    "\n",
    "if debug:\n",
    "    rows = debug_rows\n",
    "else:\n",
    "    rows = None\n",
    "\n",
    "output_dir = r\"data/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "euc    = pd.read_csv(r\"data\\enrollment_all.tsv\", sep='\\t', header=0, names=['eid','uid', 'cid'])\n",
    "\n",
    "labels = pd.read_csv(r\"data\\truth_train.tsv\", sep='\\t', header=None, names=['eid','churn_label'])\n",
    "\n",
    "data = pd.read_csv(r\"data\\JoinedLogs_TrainTest.tsv\", sep='\\t', nrows=rows)\n",
    "data['cat'].fillna('Empty', inplace=True)\n",
    "\n",
    "results = Featurize(data)\n",
    "\n",
    "results.to_csv(output_dir + \"corpus_v1.tsv\", sep='\\t', index=False)\n",
    "results = labels.merge(results, how='left')\n",
    "results.fillna(0, inplace=True)\n",
    "results.drop(['cid','uid'], axis=1, inplace=True)\n",
    "results.to_csv(output_dir + \"corpus_v2.tsv\", sep='\\t', index=False)\n",
    "\n",
    "# debug_outputs\n",
    "for key, vector in output.items():\n",
    "    vector.to_csv(output_dir + \"output_\" + key + \".tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels1  =  pd.read_csv(r\"data\\truth_train.tsv\", sep='\\t')\n",
    "results1 =  pd.read_csv(r\"data\\corpus_v1.tsv\", sep='\\t')\n",
    "results1 = labels1.merge(results1, how='left')\n",
    "results1.fillna(0, inplace=True)\n",
    "results1.drop(['cid','uid'], axis=1, inplace=True)\n",
    "results1.to_csv(output_dir + \"corpus_v4.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
